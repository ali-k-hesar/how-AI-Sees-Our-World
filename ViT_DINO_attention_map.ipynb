{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stolen from https://medium.com/@aryanjadon/visualizing-attention-in-vision-transformer-c871908d86de"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Importing Libraries\n",
    "-------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import io\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Creating a Vision Transformer\n",
    "-------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # work with diff dim tensors, not just 2D ConvNets\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "    random_tensor = keep_prob + \\\n",
    "                    torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C //\n",
    "                                  self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=[224], patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size[0], patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth decay rule\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(\n",
    "            embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(\n",
    "                math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(\n",
    "            w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "        # add the [CLS] token to the embed patch tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < len(self.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                # return attention of the last block\n",
    "                return blk(x, return_attention=True)\n",
    "\n",
    "    def get_intermediate_layers(self, x, n=1):\n",
    "        x = self.prepare_tokens(x)\n",
    "        # we return the output tokens from the `n` last blocks\n",
    "        output = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if len(self.blocks) - i <= n:\n",
    "                output.append(self.norm(x))\n",
    "        return output\n",
    "\n",
    "\n",
    "class VitGenerator(object):\n",
    "    def __init__(self, name_model, patch_size, device, evaluate=True, random=False, verbose=False):\n",
    "        self.name_model = name_model\n",
    "        self.patch_size = patch_size\n",
    "        self.evaluate = evaluate\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        self.model = self._getModel()\n",
    "        self._initializeModel()\n",
    "        if not random:\n",
    "            self._loadPretrainedWeights()\n",
    "\n",
    "    def _getModel(self):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"[INFO] Initializing {self.name_model} with patch size of {self.patch_size}\")\n",
    "        if self.name_model == 'vit_tiny':\n",
    "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "        elif self.name_model == 'vit_small':\n",
    "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "        elif self.name_model == 'vit_base':\n",
    "            model = VisionTransformer(patch_size=self.patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "                                      qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "        else:\n",
    "            raise f\"No model found with {self.name_model}\"\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _initializeModel(self):\n",
    "        if self.evaluate:\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _loadPretrainedWeights(self):\n",
    "        if self.verbose:\n",
    "            print(\"[INFO] Loading weights\")\n",
    "        url = None\n",
    "        if self.name_model == 'vit_small' and self.patch_size == 16:\n",
    "            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "\n",
    "        elif self.name_model == 'vit_small' and self.patch_size == 8:\n",
    "            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n",
    "\n",
    "        elif self.name_model == 'vit_base' and self.patch_size == 16:\n",
    "            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
    "\n",
    "        elif self.name_model == 'vit_base' and self.patch_size == 8:\n",
    "            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
    "\n",
    "        if url is None:\n",
    "            print(\n",
    "                f\"Since no pretrained weights have been found with name {self.name_model} and patch size {self.patch_size}, random weights will be used\")\n",
    "\n",
    "        else:\n",
    "            state_dict = torch.hub.load_state_dict_from_url(\n",
    "                url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
    "            self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def get_last_selfattention(self, img):\n",
    "        return self.model.get_last_selfattention(img.to(self.device))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Creating Visualization Functions\n",
    "---------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def transform(img, img_size):\n",
    "    img = transforms.Resize(img_size)(img)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_predict(model, img, img_size, patch_size, device):\n",
    "    img_pre = transform(img, img_size)\n",
    "    attention = visualize_attention(model, img_pre, patch_size, device)\n",
    "    plot_attention(img, attention)\n",
    "\n",
    "\n",
    "def visualize_attention(model, img, patch_size, device):\n",
    "    # make the image divisible by the patch size\n",
    "    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - \\\n",
    "           img.shape[2] % patch_size\n",
    "    img = img[:, :w, :h].unsqueeze(0)\n",
    "\n",
    "    w_featmap = img.shape[-2] // patch_size\n",
    "    h_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n",
    "\n",
    "    nh = attentions.shape[1]  # number of head\n",
    "\n",
    "    # keep only the output patch attention\n",
    "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "    attentions = nn.functional.interpolate(attentions.unsqueeze(\n",
    "        0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "    return attentions\n",
    "\n",
    "\n",
    "def plot_attention(img, attention):\n",
    "    n_heads = attention.shape[0]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    text = [\"Original Image\", \"Head Mean\"]\n",
    "    for i, fig in enumerate([img, np.mean(attention, 0)]):\n",
    "        if i:\n",
    "            plt.imsave(f'_results/dino_S_attn_avg.jpg', fig, cmap='inferno')\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.imshow(fig, cmap='inferno')\n",
    "        plt.title(text[i])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n_heads):\n",
    "        plt.imsave(f'_results/dino_S_attn_head_{i+1:02d}.jpg', attention[i], cmap='inferno')\n",
    "        plt.subplot(n_heads//3, 3, i+1)\n",
    "        plt.imshow(attention[i], cmap='inferno')\n",
    "        plt.title(f\"Head n: {i+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class Loader(object):\n",
    "    def __init__(self):\n",
    "        self.uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "        self._start()\n",
    "\n",
    "    def _start(self) -> None:\n",
    "        display(self.uploader)\n",
    "\n",
    "    def getLastImage(self):\n",
    "        try:\n",
    "            for uploaded_filename in self.uploader.value:\n",
    "                uploaded_filename = uploaded_filename\n",
    "            img = Image.open(io.BytesIO(bytes(self.uploader.value[uploaded_filename]['content'])))\n",
    "\n",
    "            return img\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def saveImage(self, path):\n",
    "        with open(path, 'wb') as output_file:\n",
    "            for uploaded_filename in self.uploader.value:\n",
    "                content = self.uploader.value[uploaded_filename]['content']\n",
    "                output_file.write(content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4: Visualizing Images\n",
    "-------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "name_model = 'vit_small'\n",
    "patch_size = 8\n",
    "\n",
    "model = VitGenerator(name_model, patch_size,\n",
    "                     device, evaluate=True, random=False, verbose=True)\n",
    "\n",
    "# ! wget \"https://github.com/aryan-jadon/Medium-Articles-Notebooks/raw/main/Visualizing%20Attention%20in%20Vision%20Transformer/corgi_image.jpg\"\n",
    "# ! wget \"https://github.com/aryan-jadon/Medium-Articles-Notebooks/raw/main/Visualizing%20Attention%20in%20Vision%20Transformer/orange_cat.jpg\"\n",
    "\n",
    "# Visualizing Dog Image\n",
    "path = 'pics/corgi_image.jpg'\n",
    "img = Image.open(path)\n",
    "factor_reduce = 2\n",
    "img_size = tuple(np.array(img.size[::-1]) // factor_reduce)\n",
    "visualize_predict(model, img, img_size, patch_size, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}