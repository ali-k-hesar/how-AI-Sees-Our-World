{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from skimage import transform\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ModifyModel(nn.Module):\n",
    "    # Style1: for models that containing 'avgpool' and 'fc' at the end of model.names_children()\n",
    "    # Style2: for models that containing 'features', 'avgpool' and 'classifier' in model.names_children()\n",
    "\n",
    "    def __init__(self, model, name: str = '') -> None:\n",
    "        super(ModifyModel, self).__init__()\n",
    "        self.model = deepcopy(model)\n",
    "        try:\n",
    "            self.weight_matrix = model.fc.weight.detach().clone()\n",
    "            # print(name + ':' + \"Style1\")\n",
    "        except:\n",
    "            # print(name + ':' + \"Style2\")\n",
    "            self.weight_matrix = model.classifier[-1].weight.detach().clone()\n",
    "        self.model.avgpool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1) if not name.startswith(\"vgg\") else (7, 7)),\n",
    "            nn.Flatten())\n",
    "        self.name = name\n",
    "        self.final_activation = [idx-1 for idx, (name, _) in enumerate(model.named_children()) if name == 'avgpool'][0]\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        activation_list = []\n",
    "        for name, child in self.model.named_children():\n",
    "            if name == 'features':\n",
    "                for seq in child:\n",
    "                    x = seq(x)\n",
    "                    activation_list.append(x)\n",
    "                continue\n",
    "            x = child(x)\n",
    "            activation_list.append(x)\n",
    "        return activation_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions\n",
    "----------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def class_activation_map(input_path,\n",
    "                         input_model,\n",
    "                         weight_matrix,\n",
    "                         visualize: bool = True,\n",
    "                         device : str = 'cuda',\n",
    "                         mean_vector: tuple or list = (0.485, 0.456, 0.406),\n",
    "                         std_vector: tuple or list = (0.229, 0.224, 0.225)):\n",
    "\n",
    "    # Modify mean and std vectors for element-wise operation with image array\n",
    "    mean_vector = np.array(mean_vector)[..., None, None]\n",
    "    std_vector = np.array(std_vector)[..., None, None]\n",
    "\n",
    "    # Import and normalize image\n",
    "    raw_image = read_image(input_path) / 255.\n",
    "    input_tensor = ((raw_image - mean_vector) / std_vector).unsqueeze(0).float().to(device)\n",
    "\n",
    "    # Get model results\n",
    "    input_model.to(device)\n",
    "    with torch.no_grad():\n",
    "        activations = input_model(input_tensor)\n",
    "\n",
    "    # Extract last layer weights specifically associated with predicted class node\n",
    "    weight_vector = weight_matrix[torch.argmax(activations[-1])]\n",
    "    weight_vector = weight_vector.to(device)\n",
    "\n",
    "    # Get CAM based on activation-map and last layer weights\n",
    "    final_map = torch.tensordot(weight_vector, activations[input_model.final_activation], dims=([0], [1]))\n",
    "\n",
    "    # Get ready for colormap\n",
    "    c, h, w = raw_image.shape\n",
    "    color_map = TF.resize(final_map.cpu(), [h, w])[0]\n",
    "    color_map = (color_map - color_map.min()) / (color_map.max() - color_map.min())\n",
    "    color_map = plt.cm.jet(color_map)\n",
    "\n",
    "    if visualize:\n",
    "        show_image = raw_image.permute(1, 2, 0)\n",
    "        plt.imshow(show_image, alpha=0.6)\n",
    "        plt.imshow(color_map, alpha=0.4)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"CAM|Model:{input_model.name}\")\n",
    "        plt.show()\n",
    "\n",
    "    return final_map, color_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def saliency_map(input_path,\n",
    "                 input_model,\n",
    "                 device: str = 'cuda',\n",
    "                 target_label: int or float or np.array or torch.Tensor = torch.empty(0),\n",
    "                 visualize: bool = True,\n",
    "                 mean_vector: tuple or list = (0.485, 0.456, 0.406),\n",
    "                 std_vector: tuple or list = (0.229, 0.224, 0.225)):\n",
    "\n",
    "    # Check target type\n",
    "    if not isinstance(target_label, torch.Tensor):\n",
    "        target_label = torch.as_tensor(int(target_label)).unsqueeze(0)\n",
    "\n",
    "    # Modify mean and std vectors for element-wise operation with image array\n",
    "    mean_vector = np.array(mean_vector)[..., None, None]\n",
    "    std_vector = np.array(std_vector)[..., None, None]\n",
    "\n",
    "    # Load, normalize and convert image to torch.Tensor\n",
    "    raw_image = read_image(input_path) / 255.\n",
    "    input_tensor = ((raw_image - mean_vector) / std_vector).unsqueeze(0).float().to(device).requires_grad_()\n",
    "\n",
    "    # loss function is needed for computing loss and derivative with respect to input image\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # push model to eval mode and make all parameters requires_grad to false (only input image needs gradient)\n",
    "    input_model.eval()\n",
    "    for p in input_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    input_model.to(device)\n",
    "\n",
    "    # Forward: use model output as target! (in model we trust)\n",
    "    y_hat = input_model(input_tensor)[-1]\n",
    "    y_true = torch.argmax(y_hat).unsqueeze(0) if not target_label.numel() else target_label\n",
    "    loss = criterion(y_hat, y_true)\n",
    "\n",
    "    # Compute gradient of loss with respect to input image\n",
    "    input_grad = torch.autograd.grad(loss, input_tensor)[0][0]\n",
    "\n",
    "    # Only magnitude of gradients are needed\n",
    "    input_grad.abs_()\n",
    "\n",
    "    # Normalize gradients for visualization\n",
    "    input_grad = ((input_grad - input_grad.min()) / (input_grad.max() - input_grad.min())).detach().cpu()\n",
    "\n",
    "    if visualize:\n",
    "        plt.imshow(input_grad.sum(0), alpha=0.8, cmap='hot')\n",
    "        plt.imshow(raw_image.permute(1, 2, 0), alpha=0.2)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"SaliencyMap|Model:{input_model.name}\")\n",
    "        plt.show()\n",
    "\n",
    "    return input_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# index of activations layer from each model that we want to use for GradCAM\n",
    "target_dict = {'resnet18': [1, 3, 5], 'resnet50': [1, 3, 5], 'resnet101': [1, 3, 5], 'vgg16': [8, 16, 22],\n",
    "               'googlenet': [2, 8, 15], 'efficientnet_b4': [1, 2, 4], 'efficientnet_b7': [1, 3, 5], 'mobilenet_v3': [1, 4, 11]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grad_cam(input_path,\n",
    "             input_model,\n",
    "             device: str = 'cuda',\n",
    "             target_label: int or float or np.array or torch.Tensor = torch.empty(0),\n",
    "             visualize: bool = True,\n",
    "             mean_vector: tuple or list = (0.485, 0.456, 0.406),\n",
    "             std_vector: tuple or list = (0.229, 0.224, 0.225)):\n",
    "\n",
    "    # Check target type\n",
    "    if not isinstance(target_label, torch.Tensor):\n",
    "        target_label = torch.as_tensor(int(target_label)).unsqueeze(0)\n",
    "\n",
    "    # Modify mean and std vectors for element-wise operation with image array\n",
    "    mean_vector = np.array(mean_vector)[..., None, None]\n",
    "    std_vector = np.array(std_vector)[..., None, None]\n",
    "\n",
    "    # Load, normalize and convert image to torch.Tensor\n",
    "    raw_image = read_image(input_path) / 255.\n",
    "    input_tensor = ((raw_image - mean_vector) / std_vector).unsqueeze(0).float().to(device)\n",
    "\n",
    "    # Loss value is needed for calculating derivatives\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # push model to eval mode and make parameters requires_grad to True because we want derivative of middle activations\n",
    "    input_model.to(device)\n",
    "    input_model.eval()\n",
    "    for param in input_model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Forward: if no target-label we use model output as target! (in model we trust)\n",
    "    activations = input_model(input_tensor)\n",
    "    y_true = torch.argmax(activations[-1]).unsqueeze(0) if not target_label.numel() else target_label\n",
    "    loss = criterion(activations[-1], y_true)\n",
    "\n",
    "    # Compute gradient of loss with respect to activations\n",
    "    target_activations = [activations[i] for i in target_dict[input_model.name]]\n",
    "\n",
    "    # # Option1: retain non-leaf activation gradients during backward\n",
    "    # for each in target_activations:\n",
    "    #     each.retain_grad()\n",
    "    # loss.backward()\n",
    "    #\n",
    "    # # Averaging derivatives in spacial dimensions(height and width) and create a weight vector\n",
    "    # # torch.where(i>0, i , torch.tensor(0, device=device, dtype=torch.float))\n",
    "    # target_grads = [torch.mean(torch.abs(i.grad), dim=(0, 2, 3)) for i in target_activations]\n",
    "    # for param in input_model.parameters():\n",
    "    #     param.grad.zero_()\n",
    "    # for act in target_activations:\n",
    "    #     act.grad.zero_()\n",
    "\n",
    "    # Option2: Use autograd\n",
    "    target_grads = [torch.autograd.grad(loss, i, retain_graph=True)[0] for i in target_activations]\n",
    "    # Averaging derivatives in spacial dimensions(height and width) and create a weight vector\n",
    "    # torch.where(i>0, i , torch.tensor(0, device=device, dtype=torch.float))\n",
    "    target_grads = [torch.mean(torch.abs(i), dim=(0, 2, 3)) for i in target_grads]\n",
    "\n",
    "    # Get GradCAM based on activation-maps and their derivatives\n",
    "    final_maps = []\n",
    "    for m, g in zip(target_activations, target_grads):\n",
    "        final_maps.append(torch.tensordot(m, g, dims=([1], [0])).squeeze().detach().cpu())\n",
    "\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        show_image = raw_image.permute(1, 2, 0)\n",
    "        for idx, each_map in enumerate(final_maps):\n",
    "            plt.subplot(1, len(final_maps), idx+1)\n",
    "            plt.imshow(transform.resize(each_map, output_shape=show_image.shape[:-1]), alpha=0.8, cmap='hot')\n",
    "            plt.imshow(show_image, alpha=0.2)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Activation:{target_dict[input_model.name][idx]:02d}|Model:{input_model.name}\")\n",
    "        plt.show()\n",
    "\n",
    "    return final_maps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_model = models.resnet50(pretrained=True)\n",
    "mod_model = ModifyModel(base_model, name='resnet50')\n",
    "image_path = \"pics/golden_retriever_1.jpeg\"\n",
    "\n",
    "print(\"Grad-Cam\")\n",
    "_ = grad_cam(image_path, mod_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Saliency-Map\")\n",
    "_ = saliency_map(image_path, mod_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Class-Activation-Map\")\n",
    "_ = class_activation_map(image_path, mod_model, mod_model.weight_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Group Visualization\n",
    "--------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet101 = models.resnet101(pretrained=True)\n",
    "googlenet = models.googlenet(pretrained=True)\n",
    "vgg16 = models.vgg16(pretrained=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model list for CAM\n",
    "name_list = ['resnet18', 'resnet50', 'resnet101', 'googlenet']\n",
    "root_image_path = \"pics/samples\"\n",
    "num_samples = 10\n",
    "sample_list = [os.path.join(root_image_path, i) for i in os.listdir(root_image_path)][:num_samples]\n",
    "print(f\"num_samples: {num_samples}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a 3x10 subplot figure\n",
    "fig, axs = plt.subplots(len(name_list), len(sample_list), figsize=(35, 15))\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "flatten_axs = axs.flatten()\n",
    "\n",
    "# Loop over each image and plot it on its corresponding subplot\n",
    "OUTPUT_SHAPE = (224, 224, 3)\n",
    "for i, ax_list in enumerate(axs):\n",
    "    cam_model = ModifyModel(eval(name_list[i]), name=name_list[i])\n",
    "    for j, ax in enumerate(ax_list):\n",
    "        _, cam_output = class_activation_map(sample_list[j], cam_model, cam_model.weight_matrix, visualize=False)\n",
    "        ax.imshow(transform.resize(plt.imread(sample_list[j]), OUTPUT_SHAPE), alpha=0.9)\n",
    "        ax.imshow(transform.resize(cam_output[:, :, :-1], OUTPUT_SHAPE), alpha=0.4)\n",
    "        ax.axis('off')\n",
    "        if j == 0:\n",
    "            ax.text(-0.5, 0.5, name_list[i], transform=ax.transAxes, fontsize=18, fontweight='bold')\n",
    "        if i == 0:\n",
    "            ax.text(0.5, 1.1, f\"sample-{j+1:02d}\", transform=ax.transAxes,\n",
    "                    fontsize=18, va='center', ha='center', fontweight='bold')\n",
    "\n",
    "fig.set_facecolor((1., 1., 1.))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model List for Grad-CAM\n",
    "name_list = ['resnet18', 'resnet50', 'resnet101', 'googlenet', 'vgg16']\n",
    "image_path = \"pics/golden_retriever_1.jpeg\"\n",
    "input_image = plt.imread(image_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a 3x10 subplot figure\n",
    "fig, axs = plt.subplots(3, len(name_list), figsize=(12, 8))\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "flatten_axs = axs.flatten()\n",
    "\n",
    "# Loop over each image and plot it on its corresponding subplot\n",
    "OUTPUT_SHAPE = (224, 224, 3)\n",
    "total_cams = []\n",
    "for n in range(len(name_list)):\n",
    "    cam_model = ModifyModel(eval(name_list[n]), name=name_list[n])\n",
    "    cam_outputs = grad_cam(image_path, cam_model, visualize=False)\n",
    "    total_cams.append(cam_outputs)\n",
    "\n",
    "for i, ax_list in enumerate(axs):\n",
    "    for j, ax in enumerate(ax_list):\n",
    "        ax.imshow(transform.resize(total_cams[j][i], output_shape=input_image.shape[:-1]), alpha=0.9, cmap='hot')\n",
    "        ax.imshow(input_image, alpha=0.1)\n",
    "        ax.axis('off')\n",
    "        if j == 0:\n",
    "            ax.text(-0.5, 0.5, f\"Map:{i+1:02d}\", transform=ax.transAxes, fontsize=12, fontweight='bold')\n",
    "        if i == 0:\n",
    "            ax.text(0.5, 1.1, name_list[j], transform=ax.transAxes,\n",
    "                    fontsize=12, va='center', ha='center', fontweight='bold')\n",
    "\n",
    "fig.set_facecolor((1., 1., 1.))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}