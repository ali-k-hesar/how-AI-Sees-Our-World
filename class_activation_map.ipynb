{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from skimage.transform import resize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Function\n",
    "---------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def resize_image(image, output_shape):\n",
    "    # Get the original image shape\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Compute the ratio of the new image size to the old image size\n",
    "    ratio = min(output_shape[0] / h, output_shape[1] / w)\n",
    "\n",
    "    # Compute the new image size with the same aspect ratio as the original image\n",
    "    new_h, new_w = int(h * ratio), int(w * ratio)\n",
    "\n",
    "    # Resize the image using the scikit-image library\n",
    "    resized_image = resize(image, (new_h, new_w))\n",
    "\n",
    "    # Create an output array with the desired output shape\n",
    "    output = np.ones(output_shape, dtype=resized_image.dtype)\n",
    "\n",
    "    # Compute the padding values\n",
    "    pad_h = (output_shape[0] - new_h) // 2\n",
    "    pad_w = (output_shape[1] - new_w) // 2\n",
    "\n",
    "    # Copy the resized image into the output array\n",
    "    output[pad_h:pad_h+new_h, pad_w:pad_w+new_w] = resized_image\n",
    "\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CAMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_model, model_name: str = '', device: str = 'cuda') -> None:\n",
    "        super(CAMModel, self).__init__()\n",
    "        self.model = deepcopy(input_model)\n",
    "        self.model.avgpool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten())\n",
    "        try:\n",
    "            self.weight_matrix = input_model.fc.weight.detach().clone().to(device)\n",
    "        except:\n",
    "            self.weight_matrix = input_model.classifier[-1].weight.detach().clone().to(device)\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_cam(self, input_path, visualize: bool = False,\n",
    "                mean_vector: tuple or list = (0.485, 0.456, 0.406),\n",
    "                std_vector: tuple or list = (0.229, 0.224, 0.225)): # imageNet mean and std for each channel\n",
    "\n",
    "        mean_vector = np.array(mean_vector)[..., None, None]\n",
    "        std_vector = np.array(std_vector)[..., None, None]\n",
    "\n",
    "        # Import and normalize image\n",
    "        raw_image = read_image(input_path) / 255.\n",
    "        input_image = ((raw_image - mean_vector) / std_vector).unsqueeze(0).float().to(self.device)\n",
    "\n",
    "        # Get model results\n",
    "        with torch.no_grad():\n",
    "            activation_map, final_output = self.forward(input_image)\n",
    "\n",
    "        # Extract last layer weights specifically associated with predicted class node\n",
    "        weight_vector = self.weight_matrix[torch.argmax(final_output)]\n",
    "\n",
    "        # Get CAM based on activation-map and last layer weights\n",
    "        final_map = torch.tensordot(weight_vector, activation_map, dims=([0], [1]))\n",
    "\n",
    "        # Get ready for colormap\n",
    "        c, h, w = raw_image.shape\n",
    "        color_map = TF.resize(final_map, [h, w])[0].cpu()\n",
    "        color_map = (color_map - color_map.min()) / (color_map.max() - color_map.min())\n",
    "        color_map = plt.cm.jet(color_map)\n",
    "\n",
    "        if visualize:\n",
    "            show_image = raw_image.permute(1, 2, 0)\n",
    "            plt.imshow(show_image, alpha=0.6)\n",
    "            plt.imshow(color_map, alpha=0.4)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"{self.model_name}:CAM\")\n",
    "            plt.show()\n",
    "\n",
    "        return final_map, color_map\n",
    "\n",
    "    # Modify input_model to return desired activations (feature map) in addition to final output vector (logits)\n",
    "    def forward(self, x):\n",
    "        activation = None\n",
    "        for name, child in self.model.named_children():\n",
    "            if name == 'avgpool':\n",
    "                activation = x\n",
    "            x = child(x)\n",
    "        return activation, x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Models\n",
    "------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resnet101 = models.resnet101(pretrained=True)\n",
    "for param in resnet101.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "googlenet = models.googlenet(pretrained=True)\n",
    "for param in googlenet.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regnet = models.regnet_x_400mf(pretrained=True)\n",
    "for param in regnet.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Results\n",
    "-----------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sample Data\n",
    "root_image_path = \"pics/samples\"\n",
    "num_samples = 10\n",
    "sample_list = [os.path.join(root_image_path, i) for i in os.listdir(root_image_path)][:num_samples]\n",
    "print(f\"num_samples: {num_samples}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model List\n",
    "name_list = ['resnet18', 'resnet101', 'googlenet', 'regnet']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Single Image\n",
    "source_path = sample_list[0]\n",
    "for i, each_model in enumerate(name_list):\n",
    "    cam_model = CAMModel(eval(each_model), model_name=each_model, device ='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    cam_model.get_cam(source_path, visualize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a 3x10 subplot figure\n",
    "fig, axs = plt.subplots(len(name_list), len(sample_list), figsize=(35, 15))\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "flatten_axs = axs.flatten()\n",
    "\n",
    "# Loop over each image and plot it on its corresponding subplot\n",
    "OUTPUT_SHAPE = (224, 224, 3)\n",
    "for i, ax_list in enumerate(axs):\n",
    "    cam_model = CAMModel(eval(name_list[i]), model_name=name_list[i], device ='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for j, ax in enumerate(ax_list):\n",
    "        _, cam_output = cam_model.get_cam(sample_list[j], visualize=False)\n",
    "        ax.imshow(resize_image(plt.imread(sample_list[j]), OUTPUT_SHAPE), alpha=0.9)\n",
    "        ax.imshow(resize_image(cam_output[:, :, :-1], OUTPUT_SHAPE), alpha=0.4)\n",
    "        # ax.imshow(plt.imread(sample_list[j]) / 255. * 0.6 + cam_output[:, :, :-1] * 0.4)\n",
    "        ax.axis('off')\n",
    "        if j == 0:\n",
    "            ax.text(-0.5, 0.5, name_list[i], transform=ax.transAxes, fontsize=18, fontweight='bold')\n",
    "        if i == 0:\n",
    "            ax.text(0.5, 1.1, f\"sample-{j+1:02d}\", transform=ax.transAxes,\n",
    "                    fontsize=18, va='center', ha='center', fontweight='bold')\n",
    "\n",
    "fig.set_facecolor((1., 1., 1.))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}